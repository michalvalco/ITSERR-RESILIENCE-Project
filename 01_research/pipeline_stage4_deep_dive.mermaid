%%{init: {'theme': 'base', 'themeVariables': {'primaryColor': '#fce4ec', 'primaryTextColor': '#1a1a2e', 'primaryBorderColor': '#c62828', 'lineColor': '#0f3460', 'secondaryColor': '#e8f5e9', 'tertiaryColor': '#e3f2fd', 'noteBkgColor': '#fff9c4', 'noteTextColor': '#333', 'noteBorderColor': '#f9a825'}}}%%

flowchart TD

    %% ===== INPUT =====
    subgraph INPUT["<b>Pipeline Input</b>"]
        IN1["ğŸ“¥ BIOES-tagged plaintext<br/><b>cas_to_bioes.py</b><br/><i>Source: INCEpTION export (ZIP-in-ZIP)<br/>Format: UIMA CAS XMI â†’ one token/line<br/>Multi-type labels: BIB, PAT, REF, AN, ...<br/>Feature: Tipo field â†’ get_annotation_type_label()</i>"]
    end

    %% ===== LAYER 1: RULES =====
    subgraph L1["<b>Layer 1: Rule-Based Detection</b><br/>annotate_by_rule.py"]
        R1["ğŸ” Trie + Regex Engine<br/><i>Pattern matching for structured references</i>"]
        R1A["<b>CIC patterns:</b><br/><i>NUM.q.NUM, â€” canon law format</i>"]
        R1B["<b>Protestant patterns ğŸ”§:</b><br/><i>Matt. NUM,NUM â€” biblical<br/>Aug. de civ. Dei NUM.NUM â€” patristic<br/>CA Art. NUM â€” confessional</i>"]
        R1 --- R1A
        R1 --- R1B
    end

    %% ===== LAYER 2: ABBREVIATIONS =====
    subgraph L2["<b>Layer 2: Abbreviation Dictionary</b><br/>annotate_by_abbreviations.py"]
        AB1["ğŸ“– Dictionary Lookup<br/><i>Known abbreviation sequences<br/>matched against token stream</i>"]
        AB1A["<b>CIC dictionary:</b><br/><i>C. = Causa, X = Liber Extra,<br/>D. = Distinctio, etc.</i>"]
        AB1B["<b>Protestant dictionary ğŸ”§:</b><br/><i>Matt. = Matthaeus, Aug. = Augustinus,<br/>CA = Confessio Augustana,<br/>CS = Cithara Sanctorum, etc.<br/>â† expansion_log from normalize_text.py</i>"]
        AB1 --- AB1A
        AB1 --- AB1B
    end

    %% ===== LAYER 3: MATCH =====
    subgraph L3["<b>Layer 3: Trie Match + Statistical Gap Prediction</b><br/>annotate_by_match.py"]
        M1["ğŸŒ³ Trie Exact Match<br/><i>Previously seen patterns<br/>from training data</i>"]
        M2["ğŸ“Š PREPOST Gap Prediction<br/><i>Fills gaps within mean Â± 2Ïƒ<br/>of known pattern lengths<br/>pre_post_len = 3 tokens</i>"]
        M1 --> M2
    end

    %% ===== LAYER 4: CRF =====
    subgraph L4["<b>Layer 4: CRF Machine Learning</b><br/>annotate_by_crfsuite.py"]
        direction TB
        CRF1["ğŸ¤– CRF Model<br/><b>sklearn-crfsuite</b><br/><i>Algorithm: L-BFGS<br/>CV: 5-fold, 100 iterations<br/>Scoring: F1 macro<br/>Model size: 1.1 MB (pickle)</i>"]
        CRF2["<b>Feature window: Â±6 tokens</b><br/><i>Per token: bias, raw, lowercase,<br/>isupper, istitle, isdigit,<br/>BOS/EOS markers, NUM normalisation<br/>N-grams: bi+tri+skip over 13 tokens</i>"]
        CRF3["âš ï¸ <b>No character-level features</b><br/><i>Gap for historical orthography:<br/>Å¿/s, ij/j, cz/Ä, w/v, ÃŸ/ss<br/>ğŸ”§ Proposed addition for adaptation</i>"]
        CRF4["ğŸ“Š Marginal Probabilities<br/><i>predict_marginals() â†’<br/>per-token confidence scores<br/>âš ï¸ May need calibration<br/>(Platt scaling / isotonic regression)</i>"]
        CRF1 --- CRF2
        CRF2 --- CRF3
        CRF1 --> CRF4
    end

    %% ===== LAYER 5: STRUCTURE =====
    subgraph L5["<b>Layer 5: Structural Parsing</b><br/>annotate_chapter.py / annotate_title.py / annotate_lemma.py"]
        ST1["ğŸ“‘ Chapter Detection<br/><i>Regex for document structure<br/>CIC: 'X N.N title' pattern<br/>ğŸ”§ Protestant: 'Caput N: ...' etc.</i>"]
        ST2["ğŸ“Œ Title Detection<br/><i>Section headers, rubrics</i>"]
        ST3["âœï¸ Lemma Detection<br/><i>Glossed terms positioned<br/>relative to commentary text</i>"]
        ST1 --- ST2 --- ST3
    end

    %% ===== MERGE =====
    subgraph MERGE["<b>Layer 6: Merge + Post-Process</b><br/>merge_annotations.py + post_process.py"]
        MG1["ğŸ”€ <b>First-Method-Wins Merge</b><br/><i>Priority: L1 > L2 > L3 > L4 > L5<br/>Rules override CRF by design<br/>= Precision over recall</i>"]
        MG2["ğŸ·ï¸ <b>mark_source Provenance</b><br/><i>Every annotation tagged:<br/>RULE| Â· ABBREVIATION| Â· MATCH|<br/>PREPOST| Â· CRF| Â· SOURCE|</i><br/><i>Preserved in INCEpTION Tipo field</i>"]
        MG3["ğŸ”§ Post-Processing<br/><i>CIC: single 'ff.' correction<br/>ğŸ”§ Protestant: TBD from error analysis</i>"]
        MG1 --> MG2 --> MG3
    end

    %% ===== EPISTEMOLOGICAL CLASSIFICATION =====
    subgraph EPIST["<b>Epistemological Classification</b><br/><i>Dual-path: method consensus + pattern quality</i>"]
        direction TB
        EP0["<b>Path A: Method Consensus</b><br/><i>How many layers detected<br/>the same annotation?</i>"]
        EP1["<b>Path B: Pattern Quality + Confidence</b><br/><i>CRF: predict_marginals() â†’ confidence<br/>Rules: pattern specificity<br/>(biblical+verse vs. name-only)</i>"]
        EP0 --- EP1

        F1["âœ… <b>FACTUAL</b><br/><i>â‰¥2 methods agree (consensus)<br/>OR unambiguous pattern â‰¥ 0.85<br/>(biblical+verse, confessional)</i><br/><b>â†’ Accept for public dataset</b>"]
        I1["ğŸ” <b>INTERPRETIVE</b><br/><i>Single method<br/>Confidence 0.70â€“0.85<br/>(name-only match, CRF-only)</i><br/><b>â†’ Flag for expert review</b>"]
        D1["ğŸ‘¤ <b>DEFERRED</b><br/><i>Methods disagree on type<br/>OR confidence < 0.70<br/>OR theological judgment</i><br/><b>â†’ Route to human annotator</b>"]

        EP1 --> F1
        EP1 --> I1
        EP1 --> D1
    end

    %% ===== OUTPUT =====
    subgraph OUTPUT["<b>Pipeline Output</b>"]
        OUT1["ğŸ“¤ Annotated BIOES<br/><i>bioes_to_cas.py â†’<br/>UIMA CAS XMI<br/>TypeSystem hardcoded in script<br/>â†’ INCEpTION for review</i>"]
        OUT2["ğŸ“Š Cross-Reference Index<br/><i>build_annotations_index.py<br/>â†’ LegalReferences.csv (CIC)<br/>ğŸ”§ â†’ TheologicalReferences.csv</i>"]
        OUT3["ğŸ“ˆ Statistics<br/><i>statistics.py<br/>Annotation counts by type,<br/>source, confidence</i>"]
        OUT1 --- OUT2 --- OUT3
    end

    %% ===== CONNECTIONS =====
    %% Architecture: each layer reads the INCEpTION ZIP independently
    %% and writes to its own output directory. Merge combines all outputs
    %% using first-method-wins priority (L1 > L2 > L3 > L4 > L5).
    INPUT --> L1
    INPUT --> L2
    INPUT --> L3
    INPUT --> L4
    INPUT --> L5
    L1 -->|"RULE| prefix Â· Priority 1"| MERGE
    L2 -->|"ABBREVIATION| prefix Â· Priority 2"| MERGE
    L3 -->|"MATCH| Â· PREPOST| prefix Â· Priority 3"| MERGE
    L4 -->|"CRF| prefix Â· Priority 4"| MERGE
    L5 -->|"SOURCE| prefix Â· Priority 5"| MERGE
    MERGE --> EPIST
    EPIST --> OUTPUT

    %% ===== INCEpTION ROUNDTRIP =====
    OUTPUT -. "Back to INCEpTION<br/>for expert review<br/>(iterative refinement)" .-> INPUT

    %% ===== STYLING =====
    style INPUT fill:#e3f2fd,stroke:#1565c0,stroke-width:2px
    style L1 fill:#ffebee,stroke:#c62828,stroke-width:2px
    style L2 fill:#fff3e0,stroke:#e65100,stroke-width:2px
    style L3 fill:#fff8e1,stroke:#f9a825,stroke-width:2px
    style L4 fill:#fce4ec,stroke:#ad1457,stroke-width:3px
    style L5 fill:#f1f8e9,stroke:#558b2f,stroke-width:2px
    style MERGE fill:#e8eaf6,stroke:#283593,stroke-width:2px
    style EPIST fill:#f3e5f5,stroke:#6a1b9a,stroke-width:3px
    style OUTPUT fill:#e0f2f1,stroke:#00695c,stroke-width:2px
